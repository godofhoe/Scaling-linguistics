{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compose N-gram corpora whose word satisfy Zipf's law."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random \n",
    "import bisect \n",
    "import math \n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from zipfgen import ZipfGenerator\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_file_generate_fake(char_num = 2, out_file =  'fake1.txt', sample_word_num = 8000,\n",
    "                            num_word_in_fake_scrip = 15000, \n",
    "                            alpha = 1.00001, noun = False):\n",
    "    \"\"\"Read \"roc2.txt\" file, and then generate a fake script satisfying Zipfs' law. All the words in \n",
    "    the output script share the same lenth char_num\n",
    "    \"\"\"\n",
    "    SAMPLE_WORD_NUM = sample_word_num\n",
    "    ALPHA = alpha\n",
    "    NUM_WORD_IN_NOV = num_word_in_fake_scrip\n",
    "    OUTPUT_FILE_NAME = out_file\n",
    "    NOUN = noun\n",
    "    CHAR_NUM = char_num\n",
    "    \n",
    "    zipf_gen =  ZipfGenerator(SAMPLE_WORD_NUM,ALPHA)\n",
    "    f =  open(\"roc2.txt\", \"r\", encoding = 'utf8')\n",
    "\n",
    "    world_list = []\n",
    "    \n",
    "    for line in f:\n",
    "        line_split = line.split(\"\\t\")\n",
    "        if NOUN:\n",
    "            if 'N' in line_split[4]:\n",
    "                world_list.append(line_split[3])\n",
    "        else:\n",
    "            #if len(line_split[3]) == CHAR_NUM:\n",
    "                world_list.append(line_split[3])\n",
    "\n",
    "    f.close()\n",
    "    \n",
    "    for item in world_list:\n",
    "        if item == \" \":\n",
    "            world_list.remove(item)\n",
    "    #######################################\n",
    "    ###these codes are optional \n",
    "    \n",
    "    tmp_list = []\n",
    "    for item in world_list:\n",
    "        for e in list(item):\n",
    "            tmp_list.append(e)\n",
    "    random.shuffle(tmp_list)\n",
    "    list_2 = []\n",
    "    tmp = ''\n",
    "    for e in tmp_list:\n",
    "        tmp = tmp + e\n",
    "        if len(tmp) == char_num:\n",
    "            list_2.append(tmp)\n",
    "            tmp = ''\n",
    "    \n",
    "    world_list = list_2\n",
    "\n",
    "    print(\"words in a corpus: \", len(world_list))\n",
    "    \n",
    "    \n",
    "    #######################################\n",
    "\n",
    "\n",
    "    print(\"word bank is successfully loaded.\")\n",
    "    \n",
    "    random.shuffle(world_list)\n",
    "    small_world_list = world_list[-SAMPLE_WORD_NUM:]\n",
    "    target_string_list = []\n",
    "\n",
    "    for i in range(NUM_WORD_IN_NOV):\n",
    "        num = zipf_gen.next()\n",
    "        w = small_world_list[num]\n",
    "        target_string_list.append(w+\" \")\n",
    "        \n",
    "    f2 = open(OUTPUT_FILE_NAME , 'w',encoding='utf8')\n",
    "\n",
    "    word_count = 0\n",
    "    for item in target_string_list:\n",
    "        if word_count < 20:\n",
    "            f2.write(item)\n",
    "            word_count += 1\n",
    "        else:\n",
    "            word_count = 0\n",
    "            f2.write(item+\"\\n\")\n",
    "    f2.close()\n",
    "    print(\"A fake script is successfully created !\")\n",
    "    print(\"--------------------\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change your parameters\n",
    "* char_num: char-gram. It decide the number of characters in single word. (假文本裡面每個單詞的字數)\n",
    "* out_file: name of your out-put fake corpus. (輸出的假文本的檔名)\n",
    "* sample_word_num: parameter of Zipf's law \n",
    "* num_word_in_fake_scrip: number of word in fake corpus\n",
    "* alpha: parameter of Zipf's law\n",
    "* noun: True/False, if you only chose nouns in roc.txt (Sinica corpus) to generate fake corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words in a corpus:  64878\n",
      "word bank is successfully loaded.\n",
      "A fake script is successfully created !\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "read_file_generate_fake(char_num = 1, out_file =  'FAKE1.txt', sample_word_num = 8000,\n",
    "                            num_word_in_fake_scrip = 20000, \n",
    "                            alpha = 1.00001, noun = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words in a corpus:  32439\n",
      "word bank is successfully loaded.\n",
      "A fake script is successfully created !\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "read_file_generate_fake(char_num = 2, out_file =  'FAKE2.txt', sample_word_num = 8000,\n",
    "                            num_word_in_fake_scrip = 20000, \n",
    "                            alpha = 1.00001, noun = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words in a corpus:  21626\n",
      "word bank is successfully loaded.\n",
      "A fake script is successfully created !\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "read_file_generate_fake(char_num = 3, out_file =  'FAKE3.txt', sample_word_num = 8000,\n",
    "                            num_word_in_fake_scrip = 20000, \n",
    "                            alpha = 1.00001, noun = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words in a corpus:  16219\n",
      "word bank is successfully loaded.\n",
      "A fake script is successfully created !\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "read_file_generate_fake(char_num = 4, out_file =  'FAKE4.txt', sample_word_num = 8000,\n",
    "                            num_word_in_fake_scrip = 20000, \n",
    "                            alpha = 1.00001, noun = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words in a corpus:  12975\n",
      "word bank is successfully loaded.\n",
      "A fake script is successfully created !\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "read_file_generate_fake(char_num = 5, out_file =  'FAKE5.txt', sample_word_num = 8000,\n",
    "                            num_word_in_fake_scrip = 20000, \n",
    "                            alpha = 1.00001, noun = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
